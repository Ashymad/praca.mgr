\documentclass[pl,12pt]{aghdpl}
% \documentclass[en,11pt]{aghdpl}  % praca w języku angielskim

% Lista wszystkich języków stanowiących języki pozycji bibliograficznych użytych w pracy.
% (Zgodnie z zasadami tworzenia bibliografii każda pozycja powinna zostać utworzona zgodnie z zasadami języka, w którym dana publikacja została napisana.)
\usepackage[english,polish]{babel}

% Użyj polskiego łamania wyrazów (zamiast domyślnego angielskiego).
\usepackage{polski}

\usepackage[utf8]{inputenc}

% Załączniki

\usepackage[toc, page]{appendix}
\renewcommand\appendixpagename{Załączniki}
\renewcommand\appendixtocname{Załączniki}

% dodatkowe pakiety

\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{float}% do umieszczenia floatów [H]
\usepackage{enumitem}
\setlist{nosep} % or \setlist{noitemsep} to leave space around whole list
\usepackage[bookmarks,hidelinks]{hyperref}

\usepackage{import}
\usepackage{bm}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{tikzscale}
\usepgfplotslibrary{external}
\tikzexternalize

\usetikzlibrary{positioning}
\usetikzlibrary{fit}
\tikzset{%
  highlight/.style={rectangle,rounded corners,fill=red!15,draw,fill opacity=0.5,thick,inner sep=0pt}
}
\newcommand{\tikzmark}[2]{\tikz[overlay,remember picture,baseline=(#1.base)] \node (#1) {#2};}
%
\newcommand{\Highlight}[1][submatrix]{%
    \tikz[overlay,remember picture]{
    \node[highlight,fit=(left.north west) (right.south east)] (#1) {};}
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
% Środowisko float do kodu źródłowego \begin{program}

\floatstyle{plaintop}
\ifcsname{chapter}\endcsname%
    \newfloat{program}{!tbh}{lop}[chapter]
\else%
    \newfloat{program}{!tbh}{lop}
\fi
\floatname{program}{Kod źr.}

% Kod poniżej powoduje, że floaty nie wylatują poza granice sekcji

\usepackage{placeins}

\ifcsname{chapter}\endcsname%
    \let\Oldchapter\chapter%
    \renewcommand{\chapter}{\FloatBarrier\Oldchapter}
\fi

\let\Oldsection\section%
\renewcommand{\section}{\FloatBarrier\Oldsection}

\let\Oldsubsection\subsection%
\renewcommand{\subsection}{\FloatBarrier\Oldsubsection}

\let\Oldsubsubsection\subsubsection%
\renewcommand{\subsubsection}{\FloatBarrier\Oldsubsubsection}

% --- < bibliografia > ---


\usepackage[
style=numeric,
sorting=none,
%
% Zastosuj styl wpisu bibliograficznego właściwy językowi publikacji.
language=autobib,
autolang=other,
% Zapisuj datę dostępu do strony WWW w formacie RRRR-MM-DD.
urldate=iso,
seconds=true,
% Nie dodawaj numerów stron, na których występuje cytowanie.
backref=false,
% Podawaj ISBN.
isbn=true,
% Nie podawaj URL-i, o ile nie jest to konieczne.
url=false,
%
% Ustawienia związane z polskimi normami dla bibliografii.
maxbibnames=3,
% Jeżeli używamy Bibera:
backend=biber
]{biblatex}

\usepackage{csquotes}
% Ponieważ `csquotes` nie posiada polskiego stylu, można skorzystać z mocno zbliżonego stylu chorwackiego.
\DeclareQuoteAlias{croatian}{polish}

\addbibresource{bibliografia.bib}

% Przecinki zamiast kropek do oddzielenia pól wpisu bibliograficznego
% i dwukropek po nazwisku autora, bez kropki na końcu
\AtBeginBibliography{
    \renewcommand\labelnamepunct{:\space}
    \renewcommand\newunitpunct{\addcomma\space}
    \renewcommand{\finentrypunct}{}
    \renewcommand{\bibopenparen}{\addcomma\addspace}
    \renewcommand{\bibcloseparen}{\addspace}
}

% Nie wyświetlaj wybranych pól.
%\AtEveryBibitem{\clearfield{note}}


% ------------------------
% --- < listingi > ---

% Użyj czcionki kroju Times.
\usepackage{newtxtext}
\usepackage{newtxmath}

\usepackage{listings}
\usepackage{jlcode}
\lstset{language=Julia}

\lstset{%
        literate={ą}{{\k{a}}}1
           {ć}{{\'c}}1
           {ę}{{\k{e}}}1
           {ó}{{\'o}}1
           {ń}{{\'n}}1
           {ł}{{\l{}}}1
           {ś}{{\'s}}1
           {ź}{{\'z}}1
           {ż}{{\.z}}1
           {Ą}{{\k{A}}}1
           {Ć}{{\'C}}1
           {Ę}{{\k{E}}}1
           {Ó}{{\'O}}1
           {Ń}{{\'N}}1
           {Ł}{{\L{}}}1
           {Ś}{{\'S}}1
           {Ź}{{\'Z}}1
           {Ż}{{\.Z}}1
}

% Ustawienia pakietu lstlisting do umieszczania kodu

\usepackage{color}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{%
  backgroundcolor=\color{white},     % choose the background color
  basicstyle=\ttfamily\footnotesize, % size of fonts used for the code
  breaklines, breakatwhitespace,     % automatic line breaking only at whitespace
  commentstyle=\color{mygreen},      % comment style
  numbers=left,
  showstringspaces=false,
  numberstyle=\tiny,
  frame=l,
  escapeinside={*@}{@*},           % if you want to add LaTeX within your code
  keywordstyle=\color{blue},         % keyword style
  stringstyle=\color{mymauve}        % string literal style
}

% ------------------------

\AtBeginDocument{%
        \renewcommand{\tablename}{Tab.}
        \renewcommand{\figurename}{Rys.}
}

% ------------------------
% --- < tabele > ---

\usepackage{array}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[flushleft]{threeparttable}

% defines the X column to use m (\parbox[c]) instead of p (`parbox[t]`)
\newcolumntype{C}[1]{>{\hsize=#1\hsize\centering\arraybackslash}X}


%---------------------------------------------------------------------------

\author{Szymon Piotr Mikulicz}

\makeatletter% Poniższe makra są wyłącznie zdefiniowane w klasie aghdpl-imir
\@ifclassloaded{aghdpl}{%

  \sex{m} % Mężczyzna - m; kobieta - cokolwiek
  \shortauthor{S.\ Mikulicz}
  \albumnum{279253}
  \address{Legionów 49, 05-220 Zielonka}

  \titlePL{Wykorzystanie uczenia maszynowego do identyfikacji metody kompresji
  sygnału akustycznego}
  \titleEN{{The use of machine learning to identify the method of acoustic
  signal compression}}

  \shorttitlePL{Identyfikacja kompresji audio} % skrócona wersja tytułu
  \shorttitleEN{Identification of audio compression}

  % rodzaj pracy bez końcówki fleksyjnej np. inżyniersk, magistersk
  \thesistypePL{magistersk}
  \thesistypeEN{master's}

  \supervisor{dr hab. inż. Bartłomiej Borkowski}

  \reviewer{prof. dr hab. inż. Jerzy Wiciak}

  \degreeprogrammePL{Inżynieria Akustyczna}
  \degreeprogrammeEN{Acoustic Engineering}

  \specialisationPL{Drgania i Hałas w Technice i Środowisku}
  \specialisationEN{Vibration and Noise in Technology and Environment}

  \graduationyear{2019}
  \years{2018/2019}
  \yearofstudy{II}
  \formPL{stacjonarne}
  \formEN{full-time}

  % zgoda na publikację pracy w internecie: t-zgoda, cokolwiek 
  % innego-brak zgody
  \agree{t}

  % praktyka (dyplomowa)
  \apprenticeship{Katedra Mechaniki i Wibroakustyki}

  \department{Department of Mechanics and Vibroacoustics}

  \facultyPL{Wydział Inżynierii Mechanicznej i Robotyki}
  \facultyEN{Faculty of Mechanical Engineering and Robotics}

  \thesisplan{% Przykładowy plan pracy, należy omówić z promotorem
    \begin{enumerate}
    \item Omówienie tematu pracy i sposobu realizacji z promotorem.
    \item Zebranie i opracowanie literatury dotyczącej tematu pracy.
    \item Zebranie i opracowanie wyników badań.
    \item Analiza wyników badań, ich omówienie i zatwierdzenie przez promotora.
    \item Opracowanie redakcyjne.
    \end{enumerate}
  }

  \summaryPL{\indent\indent%
	  {[Treść streszczenia]}
  }
  \summaryEN{\indent\indent%
	  {[Summary text]}
  }

  \acknowledgements{%
    Dziękuję mojemu promotorowi, prof.~Borkowskiemu, za cierpliwość, Stevenowi
    G.  Johnsonowi za MDCT.jl, Donaldowi E. Knuthowi za \TeX\ oraz moim
    przyjaciołom ot tak, bo mogę.
  }

  \setlength{\cftsecnumwidth}{10mm}
}{}%
\makeatother%

\date{\today}

%---------------------------------------------------------------------------
\setcounter{secnumdepth}{4}
\brokenpenalty=10000\relax

\begin{document}

\titlepages

% Ponowne zdefiniowanie stylu `plain`, aby usunąć numer strony z pierwszej strony spisu treści i poszczególnych rozdziałów.
\fancypagestyle{plain}
{%
        % Usuń nagłówek i stopkę
        \fancyhf{}
        % Usuń linie.
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}
}

\setcounter{tocdepth}{2}
{\singlespacing\tableofcontents}
\clearpage

\chapter{Wprowadzenie}
\section{Cel pracy}
\section{Zawartość pracy}
\chapter{Kompresja audio}
\section{Wprowadzenie}
W informatyce pojęcie kompresji odnosi się do procesu zmniejszenia rozmiaru
pliku. Jako że pliki multimedialne typu filmy i muzyka są dużej wielkości
{\color{red}jakiej, porównać!} bez użycia kompresji w porównaniu do innego typu
plików wykorzystywanych przez użytkowników komputera, a ponadto przesyłane są
często one przez łącze sieciowe do odbiorcy, zmniejszenie ich rozmiaru
oszczędza ogromne ilości miejsca na dysku {\color{red}jakie ilości} jak i
przepustowości sieci. Z tego powodu kompresja audio, video i obrazów jest
stosowana powszechnie, nieskompresowane pliki multimedialne używane są
wyłącznie gdy wymagany jest szybki dostęp do ich fragmentów i brak obaw o
pogorszenie jakości, na przykład podczas procesu ich edycji.

Ponadto, gdyż rodzaj kompresji stosowany do innych plików, np.\ tekstowych,
czyli kompresja bezstratna nie osiąga wystarczającego zmniejszenia rozmiaru
plików multimedialnych, z powodu wysokiej entropii tego rodzaju plików, stosuje
się metody pozwalające na dalsze zmniejszenie rozmiaru kosztem utraty
możliwości odtworzenie oryginalnego sygnału po dekompresji -- z tego powodu ten
typ kompresji nazywa się kompresją stratną.

{\color{red}może wykres z rozmiarami plików po kompresji}
\section{Kompresja bezstratna}

W procesie kompresji stratnej uzyskuje się zmniejszenie rozmiaru pliku bez
utraty informacji. Uzyskuje się to poprzez zastosowanie pewnych przekształceń
które zostaną tu wytłumaczone na przykładzie kompresji FLAC
(eng. \textit{Free Lossless Audio Codec}), z dwóch powodów: FLAC jest obecnie
najpopularniejszym formatem kompresji bezstratnej oraz posiada w pełni otwartą
specyfikację i referencyjny enkoder.

\begin{figure}[!tbh]
  \centering
  \import{./vecgraphics/}{FLAC-scheme.pdf_tex}
  \caption{Schemat kompresji bezstratnej (na przykładzie FLAC)}
  \label{fig:FLAC_scheme}
\end{figure}

Podczas procesu kompresji FLAC występują cztery główne kroki (patrz rys.
\ref{fig:FLAC_scheme}): fragmentacja, dekorelacja, modelowanie i kodowanie.
Pierwszy krok, czyli fragmentacja dzieli sygnał cyfrowy na fragmenty (eng.
\textit{blocks}), których długość wybierana jest na tyle krótka by model
stworzony w trzecim kroku był jak najbardziej efektywny i na tyle długa by
nadmiarowość (eng. \textit{overhead}) wynikająca z konieczności zapisu
parametrów zastosowanych do kompresji fragmentu była jak najmniejsza. 

W drugim kroku przeprowadzana jest dekorelacja kanałów w pliku audio, w
przypadku pliku stereofonicznego kanały prawy $r$ i lewy $l$ zamieniane są na środkowy
$m = \frac{l + r}{2}$ i boczny $s = l - r$. Pozwala to znacznie zwiększyć
poziom kompresji w dalszych krokach, ponieważ typowo duża korelacja pomiędzy
kanałami powoduje że kanał różnicowy (boczny) zawiera niewielkie wartości
amplitudy próbek.

Trzeci krok jest najbardziej istotny gdyż w nim tworzony jest model
matematyczny w taki sposób, by różnica pomiędzy sygnałem wygenerowanym przez
niego a sygnałem rzeczywistym (nazywana błędem lub residuum), wymagała
jak najmniejszej liczby bitów na próbkę (eng. \textit{bits-per-sample}).
Standard FLAC przewiduje dwa typy modeli: wielomian lub kodowanie predykcyjne
LPC (eng. \textit{Linear Predictive Coding}). Dopasowanie wielomianu wymaga
mniejszej mocy obliczeniowej lecz wynik jest mniej dokładny niż LPC.

Ostatni krok to kodowanie entropijne uzyskanego w poprzednim kroku sygnału
błędu. W kompresji FLAC wykorzystane jest kodowanie Huffmana, które
wykorzystuje nierównomierności rozkładu częstotliwości występowania sekwencji
bitów w danych, zapisując najczęściej występujące sekwencję mniejszą liczbą
bitów. Zazwyczaj związane jest to z zapisem tzw.\ słownika, który
przyporządkowuje sekwencje bitów do kodów Huffmana. Natomiast, ponieważ dane
błędu uzyskane w tym procesie posiadają rozkład Laplace'a (podwójnie
wykładniczy), możliwe jest zastosowanie szczególnego układu kodów Huffmana,
nazwanych kodami Rice'a. Własnością tych kodów jest możliwość ich wygenerowania
na podstawie pojedynczego parametru opisującego rozkład danych (parametru
Rice'a), ponadto standard FLAC pozwala na podzielenie fragmentu na części,
gdzie każda część posiada inny parametr Rice'a, co pozwala na lepsze
dopasowanie go do zmienności rozkładu w trakcie trwania sygnału.

Poprawnie działający enkoder i dekoder pozwolą otrzymać, po kompresji i
dekompresji pliku, plik w którym próbki sygnału audio będą takie same jak w
pliku oryginalnym. Z tego powodu wykrycie czy plik został poddany takiemu
procesowi niemożliwe na podstawie samego sygnału audio.

\section{Kompresja stratna}

Celem kompresji stratnej, w przeciwieństwie do bezstratnej nie jest stworzenie
mniejszego pliku który po dekompresji odtworzy sygnał wejściowy co do próbki, lecz
stworzenie pliku dużo mniejszego, który po dekompresji i odtworzeniu będzie
możliwie nierozróżnialny od oryginalnego dla większości słuchaczy. Dlatego też
ten typ kompresji jest dużo bardziej skomplikowany gdyż wykorzystuje nie tylko
te techniki co kompresja bezstratna, ale również wymaga zastosowania wiedzy
z dziedziny psychoakustyki. W związku z tym przetestowanie działania kodeku
wymaga testów odsłuchowych co znacząco spowalnia proces tworzenia nowych
rozwiązań.

\begin{figure}[!tbh]
  \centering
  \import{./vecgraphics/}{PAC-scheme.pdf_tex}
  \caption{Schemat kompresji stratnej}
  \label{fig:PAC_scheme}
\end{figure}

Na rysunku \ref{fig:PAC_scheme} przedstawiony jest uproszczony schemat typowego
enkodera PAC (eng. \textit{Perceptual Audio Coding}) wykorzystywanego w
kompresji stratnej. Posiada on cztery główne elementy: transformację
analizującą, kwantyzator, enkoder i model psychoakustyczny.

Pierwszym elementem jest transformacja analizująca, która zamienia sygnał audio
na postać którą może wykorzystać kwantyzator wraz z modelem psychakustycznym.
Każdy z opisanych w tym rozdziale kodeków wykorzystuje w tym celu miedzy innymi
zmodyfikowaną transformację kosinusową MDCT (eng. \textit{Modified Discrete
Cosine Transform}) określoną wzorem:
\begin{equation}
  X_k = \sum_{n=0}^{2N-1}x_n\cos\left[\frac{\pi}{N}
  \left(n+\frac{1}{2}+\frac{N}{2}\right)\left(k+\frac{1}{2}\right)\right].
\end{equation}
Jest to funkcja liniowa $F\colon \bm{R}^{2N} \to \bm{R}^N$, która przekształca
$2N$ liczb rzeczywistych $x_0, \dotsc, x_{2N+1}$ w $N$ liczb rzeczywistych
$x_0, \dotsc, x_{N+1}$. Posiada ona szczególną właściwość wykorzystywaną w
procesie kompresji, mianowicie większość współczynników z niej uzyskanych
posiada wartości bliskie zeru.

Aby przeprowadzić transformację na sygnale, wybierana jest długość okna $l =
2c$, gdzie $c \in \bm N$, oraz skok $h \in \bm N$. Długość okna określa długość
fragmentu sygnału $\bm s$ który zostanie pomnożony przez okno $\bm w$, a
następnie poddany transformacji, natomiast skok określa co ile próbek jest
wycinany kolejny fragment. Wynikiem całej operacji jest jest macierz $\bm X =
\begin{bmatrix}\bm x_1 & \bm x_2 & \cdots  & \bm x_k\end{bmatrix}$, stworzona z
wektorów \[\bm x_n = \text{MDCT}\left\{\bm
w\begin{bmatrix}s_{(n-1)h}\\\vdots\\s_{(n-1)h+l}\end{bmatrix}\right\},\] gdzie
okno $\bm w$ jest wektorem o długości $l$, wyznaczonym na podstawie pewnej
funkcji $w(n)$, której zadaniem jest zniwelowanie efektów brzegowych. Aby
transformacja była odwracalna, okno musi być symetryczne, tzn.\ spełniać
warunek symetryczności (\ref{eq:symm_cond}) i warunek Princena-Bradleya
(\ref{eq:p-b_cond}).
\begin{equation}\label{eq:symm_cond}
  w_n = w_{l-1-n}
\end{equation}
\begin{equation}\label{eq:p-b_cond}
  w_n^2+w_{n+\frac{l}{2}}^2 = 1
\end{equation}

Sygnał audio jest także przetwarzany przez model psychakustyczny którego
zadaniem jest przewidywanie zachowania ucha ludzkiego, w taki sposób by
określić czego potencjalny słuchacz może nie usłyszeć, a co usłyszy.
Wykorzystuje on w tym celu wiedzę o konstrukcji ucha oraz wiedzę uzyskaną
poprzez przeprowadzanie testów odsłuchowych.

Informacje uzyskane z modelu psychoakustycznego są następnie stosowanie przez
kwantyzator do kwantyzacji (ograniczenia zbioru wartości) wyniku transformacji
analizującej. Elementy które model uzna za niesłyszalne dla potencjalnego
odbiorcy zostaną silniej skwantyzowane niż te które zostaną uznane za
słyszalne. Jest to proces nieodwracalny z którego wynika ,,stratność'' tego
rodzaju kompresji, w jego wyniku dostaje się także macierz wag przez którą
trzeba pomnożyć uzyskane wartości w procesie odwrotnym.

W ostatnim kroku, podobnie jak w przypadku kompresji bezstratnej, dane uzyskane
z poprzedniego kroku są zakodowane przy użyciu kodowania entropijnego. Jest ono
bardzo skuteczne na uzyskanych danych dzięki opisanej wcześniej własności
transformaty MDCT oraz procesowi kwantyzacji, które powodują wyzerowanie się
dużej liczby współczynników uzyskanych w procesie transformacji.

\begin{figure}[!tbh]
  \centering
  \import{./vecgraphics/}{IPAC-scheme.pdf_tex}
  \caption{Schemat dekompresji stratnej}
  \label{fig:IPAC_scheme}
\end{figure}

Aby z pliku skompresowanego uzyskać sygnał audio stosuje się dekompresją (patrz
rys. \ref{fig:IPAC_scheme}), gdzie kolejno następuje: dekodowanie zakodowanych
entropijnie danych, odwrotna kwantyzacja która polega na wymnożeniu danych
przez odpowiednie wagi (ten proces jednakże nie odtwarza w pełni danych z przed
kwantyzacji lecz dane zbliżone) oraz synteza, czyli zastosowanie odwrotnych
transformat analizujących, w przypadku MDCT będzie to IMDCT (eng.
\textit{Inverse Modified Discrete Cosine Transform})

W kolejnych punktach zostaną dokładniej opisane elementy procesu kompresji
wykorzystane w pięciu popularnych kodekach.

\subsection{MP3}
Format MP3 (eng. \textit{MPEG-1 Layer 3}) został zdefiniowany w roku 1991 jako część
standardu MPEG-1 (eng. \textit{Moving Picture Experts Group Phase 1}) a
następnie rozszerzony w standardach MPEG-2 oraz MPEG-2.5. Jest to obecnie
nadal jeden z najbardziej popularnych formatów, choć od czasu jego wprowadzenia
powstały kodeki o dużo lepszych parametrach.

W tym kodeku oprócz MDCT do analizy sygnału jest zastosowany również bank
filtrów PQMF (eng. \textit{Pseudo Quadrature Mirror Filter}), jest to bank filtrów
o prawie idealnej rekonstrukcji, którego proces konstrukcji wygląda
następująca:
\begin{enumerate}
  \item Projekt prototypowego filtra dolnoprzepustowego $h(n)$ o długości $M =
    LN$, gdzie $L, M, N \in \bm Z$
  \item Filtr musi anulować aliasing w sąsiadujących podpasmach: \[
      \begin{array}{rl}
        |H(e^{i\omega})|^2 + H(e^{i\frac{\pi}{N}-\omega})|^2 = 2, &
        \quad0<|\omega|<\frac{\pi}{2N} \\
        |H(e^{i\omega})|^2 = 0, & \quad|\omega| > \frac{\pi}{N}
      \end{array}\]
  \item Filtry analizy $h_k(n)$ to modulacje kosinusowe filtra $h(n)$:\[
      h_k(n) = h(n)\cos\left[\left(k+\frac{1}{2}\right)
      \left(n-\frac{M-1}{2}\right)\frac{\pi}{N}+\phi_k\right],\]
      gdzie $k = 0,\dotsc,N-1$, a faza spełnia warunek:\[
      \phi_{k+1}-\phi_k = (2r+1)\frac{\pi}{2}.\]
  \item Bank filtrów jest ortogonalny, więc filtry syntezy są filtrami analizy
    odwróconymi w czasie:\[
      f_k(n) = h_k(M-1-n)\]
\end{enumerate}
W formatach od MPEG-1 Layer I do MPEG-1 Layer III, ten bank filtrów użyto z $N = 32$
pasmami i o $M = 512$ współczynnikach, przy czym w MP3 dodatkowo do wyjścia każdego z
pasm zaaplikowano filtr Princena-Bradleya {\color{red}odnośnik}.

MDCT w tym formacie wykorzystuje okno sinusowe (nazywane też kosinusowym) (rys.
\ref{fig:win_MP3}) o długości $N$, o następującym wzorze:
\begin{equation}\label{eq:sine_win}
  w(n) = \sin\left(\frac{\pi n}{N}\right) = \cos\left(\frac{\pi n}{N} - \frac{\pi}{2}\right).
\end{equation}

\begin{figure}[!tbh]
  \centering
  \includegraphics[width=0.5\textwidth]{plots/win_MP3.pgf}
  \caption{Okno sinusowe/kosinusowe}
  \label{fig:win_MP3}
\end{figure}

\subsection{AAC}

MDCT w tym formacie wykorzystuje okno KBD (eng.
\textit{Keiser-Bessel-derived}), jest to okno pochodne okna Keisera, stworzone
by spełniało warunki zastosowania w MDCT (\ref{eq:symm_cond},
\ref{eq:p-b_cond}). Okno Kaisera ma wzór:
\begin{equation}\label{eq:kaiser_win}
  w_k(n) = \frac{I_0\left[\pi\alpha\sqrt{1-\left(\frac{2n}{N}-1\right)^2}\right]}
  {I_0(\pi\alpha)},\quad 0 \leq n \leq N,
\end{equation}
gdzie długość okna wynosi $N+1$, $I_0$ to zerowego rzędu zmodyfikowana funkcja
Bessela pierwszego rodzaju, a $\alpha$ to parametr wpływający na kształt tego
okna. Na jego podstawie zdefiniowane jest okno Kaisera-Bessela o długości $2N$
o wzorze:
\begin{equation}\label{eq:kbd_win}
  w(n) = \begin{cases}
    \sqrt{\frac{\sum_{i=0}^{n}w_k(i)}{\sum_{i=0}{N}w_k(i)}} & \text{dla }0 \leq n \leq N \\
    \sqrt{\frac{\sum_{i=0}^{2N-1-n}w_k(i)}{\sum_{i=0}{N}w_k(i)}} & \text{dla }N \leq n \leq 2N-1 \\
    0 & \text{w innym przypadku}
  \end{cases}.
\end{equation}
AAC wykorzystuje okno KBD z parametrem $\alpha = 4$ (rys. \ref{fig:win_AAC})

\begin{figure}[!tbh]
  \centering
  \includegraphics[width=0.5\textwidth]{plots/win_AAC.pgf}
  \caption{Okno KBD, $\alpha = 4$}
  \label{fig:win_AAC}
\end{figure}

\subsection{Ogg Vorbis}

MDCT w tym formacie wykorzystuje okno zboczowe (eng. \textit{slope}) (rys.
\ref{fig:win_OGG}) o długości $N$, o wzorze:
\begin{equation}\label{eq:slope_win}
  w(n) = \sin\left(\frac{\pi}{2}\sin^2\left(\frac{\left(n +
  \frac{1}{2}\right)}{N\pi}\right)\right)
\end{equation}
\begin{figure}[!tbh]
  \centering
  \includegraphics[width=0.5\textwidth]{plots/win_OGG.pgf}
  \caption{Okno zboczowe}
  \label{fig:win_OGG}
\end{figure}

\subsection{AC-3}

MDCT w tym formacie wykorzystuje okno KBD (\ref{eq:kaiser_win},
\ref{eq:kbd_win}), z parametrem $\alpha = 5$ (rys. \ref{fig:win_AC3}).
\begin{figure}[!tbh]
  \centering
  \includegraphics[width=0.5\textwidth]{plots/win_AC3.pgf}
  \caption{Okno KBD, $\alpha = 5$}
  \label{fig:win_AC3}
\end{figure}

\subsection{WMA}

MDCT w tym formacie wykorzystuje okno sinusowe (\ref{eq:sine_win})
(rys. \ref{fig:win_MP3})

\chapter{Realizacja}
Aby stworzyć proces pozwalający na wykrycie formatu z którego sygnał audio
został dekodowany, przeanalizowano dwa podejścia, nie tylko wykorzystano
techniki uczenia maszynowego, ale ponadto, zaimplementowano algorytm który, w
przeciwieństwie do modelu, nie wymaga uprzedniego treningu.
\section{Podejście algorytmiczne}
Implementowany algorytm oparty został o algorytm opisany w
{\color{red}odnośnik}. Algorytm ten potrafi wskazać czy dany sygnał został
skompresowany przez pewien kodek audio, którego parametry: transformacja
analizująca $T(x)$, długość okna $N \in \bm N_+$, funkcja okna $w(n)$ oraz skok
$h \in \left[1, N\right]$ są znane.
Przedstawia się on następująco:
\begin{enumerate}
  \item Wycięcie $\frac{N}{2}+1$ fragmentów o długości $L = nN$, gdzie $n \in
    \bm N$, z sygnału $\bm s$,
    poczynając od indeksu $i$ do indeksu $i+\frac{N}{2}$
    \begin{equation}
      \begin{bmatrix}
        s_i \\\vdots\\ s_{i+L-1}
      \end{bmatrix},
      \begin{bmatrix}
        s_{i+1} \\\vdots\\ s_{i+L}
      \end{bmatrix}, \dotsc,
      \begin{bmatrix}
        s_{i+\frac{N}{2}} \\\vdots\\ s_{i+\frac{N}{2}+L-1}
      \end{bmatrix}
    \end{equation}
  \item Wykonanie krótkoczasowej transformacji $T(x)$ z oknem $w(n)$ o długości
    $N$ i skoku $h$ na fragmentach uzyskanych w poprzednim kroku uzyskując
    $\frac{N}{2}+1$ macierzy $\hat{\bm S}_i$
    \begin{equation}
      \hat{\bm S}_i = \underset{w(n), N, h}{T}\left(
        \begin{bmatrix}
          s_i \\\vdots\\ s_{i+L-1}
        \end{bmatrix}\right)
    \end{equation}
  \item Policzenie średniej modułów logarytmów dziesiętnych modułów macierzy
    $\hat{\bm S}_i$ uzyskując $\frac{N}{2}+1$ średnich $\left<\hat{\bm S}_i\right>$
    \begin{equation}
      \left<\hat{\bm S}_i\right> =
      \frac{1}{L}\sum_m\sum_n\left|\log_{10}
      \left(\left|\hat{S}_{i, mn}\right|\right)\right|
    \end{equation}
  \item Wyznaczenie modułów różnic pomiędzy kolejnymi średnimi uzyskując
    wektor $\bm d_i$ o długości $\frac{N}{2}$
    \begin{equation}
      \bm d_i = \begin{bmatrix}
        \left|\left<\hat{\bm S}_i\right> -
        \left<\hat{\bm S}_{i+1}\right>\right|\\
        \left|\left<\hat{\bm S}_{i+1}\right> -
        \left<\hat{\bm S}_{i+2}\right>\right|\\
        \vdots\\
        \left|\left<\hat{\bm S}_{i+\frac{N}{2}-1}\right> -
        \left<\hat{\bm S}_{i+\frac{N}{2}}\right>\right|
      \end{bmatrix}
    \end{equation}
    \item Znalezienie wartości maksymalnej $d_{i,\max}$ wektora $\bm d_i$ oraz
      indeksu $k_{i,\max}$ który posiada ta wartość w tym wektorze
      \begin{equation}
        d_{i,\max} = \max_k\left\{d_{i, k}\right\}
      \end{equation}
      \begin{equation}
        k_{i,\max} = \argmax_k\left\{d_{i,k}\right\} \iff d_{i,\max} =
        d_{i,k_{i,\max}}
      \end{equation}
    \item Powtarzanie poprzednich kroków $M$ razy, zwiększając wartość $i$ o $L$
      w każdym kroku, tak że w $m$-tym kroku $i = i_0 + (m-1)L$, gdzie $i_0$ to
      dowolny wybrany indeks sygnału $\bm s$. Wynikiem operacji są wektory
      wartości maksymalnych $\bm d_{\max}$ i odpowiadających im indeksów $\bm
      k_{\max}$ o długości $M$
      \begin{equation}
        \bm d_{\max} = \begin{bmatrix}
          d_{i_0,\max}\\
          d_{i_0+L,\max}\\
          \vdots\\
          d_{i_0+(M-1)L,\max}
        \end{bmatrix},
        \bm k_{\max} = \begin{bmatrix}
          k_{i_0,\max}\\
          k_{i_0+L,\max}\\
          \vdots\\
          k_{i_0+(M-1)L,\max}
        \end{bmatrix}
      \end{equation}
    \item Znalezienie okresu $p_{\min}$, zdefiniowanego jako największa wartość
      $p \in [3, h]$ dla której współczynnik zmienności $V_{r}(p)$ reszt $r(p,
      j)$ z dzielenia elementów wektora indeksów wartości maksymalnych $\bm
      k_{\max}$ przez $p$ osiąga wartość minimalną
      \begin{equation}
        r(p, j) = k_{\max, j}\mod p
      \end{equation}
      \begin{equation}
        \overline r(p) = \frac{1}{M}\sum_{j=1}^{M}r(p, j)
      \end{equation}
      \begin{equation}
        V_{r}(p) = \frac{1}{\overline r(p)}\sqrt{\frac{1}{M}
        \sum_{j=1}^{M}\left(r(p, j) - \overline r(p)\right)^2}
      \end{equation}
      \begin{equation}
        p_{\min} = \max\left\{\argmin_{3 \leq p \leq h}
        \left\{V_{r}(p)\right\}\right\}
      \end{equation}
    \item Wyznaczenie wektora kątów $\bm\phi_k$ poprzez pomnożenie stosunku
      reszty z dzielenia elementów wektora $\bm k_{\max}$ przez
      okres $p_{\min}$ do $p_{\min}$ przez $2\pi$
      \begin{equation}
        \bm\phi_k = \frac{2\pi}{p_{\min}}\begin{bmatrix}
          k_{\max,1}\mod p_{\min}\\
          \vdots\\
          k_{\max,M}\mod p_{\min}\\
        \end{bmatrix}
      \end{equation}
    \item Obliczenie jednoliczbowego wskaźnika $C_{T(x),w(n),N,h}$ którego
      wielkość jest wprost proporcjonalna do prawdopodobieństwa zastosowania
      kompresji o parametrach $T(x)$, $w(n)$, $N$ i $h$. Jest
      on zdefiniowany jako odległość od środka układu średniej geometrycznej
      punktów określonych w układzie współrzędnych radialnych jako pary
      $(d_{\max, j}, \phi_{k, j})$, gdzie promieniami są elementy wektora
      wartości maksymalnych $\bm d_{\max}$, natomiast kątami są odpowiadające im
      elementy wektora kątów $\bm\phi_k$
      \begin{equation}
        C_{T(x),w(n),N,h} =\frac{1}{M}\sqrt{\left(\sum_{j=1}^{M}d_{\max,j}\sin
        \phi_{k,j}\right)^2+\left(\sum_{j=1}^{M}d_{\max,j}\cos
        \phi_{k,j}\right)^2}
      \end{equation}
\end{enumerate}

Algorytm ten usprawnia poprzednie rozwiązania poprzez dodanie siódmego kroku
który automatycznie znajduje wartość $p_{\min}$ oraz modyfikuje krok trzecie
poprzez wprowadzenie średniej modułów logarytmów modułów zamiast modułu
logarytmów średniej kwadratowej, pozwalając na znacznie efektywniejszą
implementację.
\subsection{Biblioteka \textit{IOLA.jl}}
Procedury algorytmu zaimplementowano w języku programowania \textit{Julia} w
postaci biblioteki o nazwie \textit{IOLA} (eng.\ \textit{Identification Of
Lossy Audio}). Udostępnia ona dwa elementy: moduł \lstinline|Codec| i funkcję
\lstinline|analyze(signal, transform, segment_length, overlap, normalize)|.
Ponadto w module \lstinline|Codec.Utils| udostępnione są funkcje
\lstinline|findperiod(indexes, domain)| oraz
\lstinline|radiusofmean(radiuses, angles)|.

Moduł \lstinline|Codec| zawiera parametry kodeków, które następnie można
wykorzystać do analizy fragmentu audio. Udostępnione są w tym celu: funkcja
\lstinline|Codec.getparams(codec_type)|, funkcja
\lstinline|Codec.gettransform(codec_params)|, oraz enumerator
\lstinline|Codec.CodecType| zawierający obsługiwane kodeki, obecnie są to:
\lstinline|Codec.MP3|, \lstinline|Codec.AAC|, \lstinline|Codec.AC3|,
\lstinline|Codec.OGG| i \lstinline|Codec.WMA|. Aby uzyskać parametry kodeka
należy wywołać funkcję \lstinline|Codec.getparams()| na którejś wartości
enumeratora \lstinline|Codec.CodecType|, uzyskując strukturę
\lstinline|Codec.CodecParams| która zawiera pola \lstinline|transform|,
\lstinline|window|, \lstinline|length| i \lstinline|hop|, które odpowiadają
parametrom $T(x)$, $w(n)$, $N$ i $h$. Wywołanie funkcji
\lstinline|gettransform()| na uzyskanej strukturze natomiast pozwala uzyskać
krótkoczasową transformatę $\underset{w(n),N,h}{T(x)}$.

Funkcja \lstinline|analyze()| jest najważniejszą częścią biblioteki, wykonuje
ona kroki algorytmu od pierwszego do szóstego zwracając macierz $M \times 2$,
która w pierwszej kolumnie zawiera elementy wektora $\bm d_{\max}$ natomiast w
drugiej kolumnie elementy wektora $\bm k_{\max}$:
\[\begin{bmatrix}
    d_{\max,1} & k_{\max,1}\\
    \vdots & \vdots\\
    d_{\max,M} & k_{\max,M}
\end{bmatrix}.\]
Wymaga ona podania fragmentu sygnału $\bm s$ na którym chce się przeprowadzić
analizę, krótkoczasowej transformaty $\underset{w(n),N,h}{T(x)}$, długości
fragmentu $L$ oraz skoku $h$. Ponadto posiada ona opcjonalny parametr który,
jeśli ustalony na wartość \lstinline|true|, dodaje dodatkowy krok po kroku
czwartym, polegający na normalizacji wektora $\bm d_i$, czyli odjęciu średniej
$\overline{d_i}$ i podzieleniu przez odchylenie standardowe $s_{d_i}$ każdego
elementu, zastępując go wektorem znormalizowanym $\bm d_{i, \text{norm}}$ w
dalszych krokach.
\begin{equation}
  \overline{d_i} = \frac{1}{M}\sum_{j=1}^{M}d_{i,j}
\end{equation}
\begin{equation}
  s_{d_i} = \sqrt{\frac{1}{M-1}\sum_{j=1}^{M}\left(d_{i, j} -
  \overline{d_{i}}\right)},
\end{equation}
\begin{equation}
  \bm d_{i, \text{norm}} = \frac{1}{s_{d_i}}\begin{bmatrix}
    d_{i,1} - \overline{d_i}\\
    \vdots\\
    d_{i,M} - \overline{d_i}
  \end{bmatrix}
\end{equation}
Ten dodatkowy krok pozwala łatwiej porównać wyniki uzyskane dla różnych
sygnałów, lecz zmniejsza różnice pomiędzy wynikami dla różnych stopni kompresji
utrudniając tym samym ich rozróżnienie.

Funkcje pomocnicze \lstinline|findperiod()| i \lstinline|radiusofmean()|
wykonują odpowiednio siódmy i dziewiąty krok algorytmu. Pierwsza zwraca
$p_{\min}$ po podaniu jej wektora $\bm k_{\max}$ i zakresu szukania np.
$[3,h]$, natomiast druga pozwala wyliczyć $C_{T(x),w(n),N,h}$ na podstawie $\bm
d_{\max}$ i $\bm\phi_k$.

Kolejne użycie funkcji z tej biblioteki pozwala przeprowadzić analizę pliku pod
kątem konkretnego kodeka, przykład takiego użycia dla formatu MP3 przedstawia
kod źródłowy \ref{lst:IOLA_example}.

\begin{program}
  \caption{Zastosowanie biblioteki \textit{IOLA.jl} do analizy kompresji MP3}
  \label{lst:IOLA_example}
  \begin{lstlisting}
  # Zaimportowanie potrzebnych bibliotek
  using IOLA
  using IOLA.Utils
  using WAV
  # Wczytanie parametrów kodeka i danych audio oraz ustalenie długości segmentu
  params = Codec.getparams(Codec.MP3)
  transform = Codec.gettransform(params)
  L = 10*params.length
  (s, fs, nbits, opt) = wavread("audio.wav")
  # Analiza sygnału
  dk = analyze(y, transform, L, params.hop)
  d = dk[:,1]
  k = dk[:,2]
  # Wyznaczenie wskaźnika *@$\color{mygreen}C_{T(x),w(n),N,h}$@*
  pmin = findperiod(k, 3:params.hop)
  phi = 2pi*mod.(k, pmin)./pmin
  C = radiusofmean(d, phi)
  \end{lstlisting}
\end{program}

Kluczowe w implementacji było wykorzystanie jak najefektywniejszych procedur, w
szczególności dla MDCT i średniej modułów logarytmów modułów, których
implementacje opisano w kolejnych punktach, w celu skrócenia
długiego czasu przetwarzania. Ponadto wykorzystano wbudowane w język
\textit{Julia} możliwości wielowątkowości, które pozwalają na rozłożenie kroków
od pierwszego do piątego na wiele rdzeni procesora jako że są one od siebie
niezależne.
\subsection{Średnia modułów logarytmów modułów}
Mając macierz $\bm X$ o wymiarach $M \times N$ średnia modułów logarytmów modułów
(od teraz nazywana po prostu ,,średnią'') jest to suma wartości absolutnych z
logarytmu o podstawie $q$ z wartości absolutnych wszystkich elementów tej
macierzy.
\begin{equation}
  \left<X\right> =
  \frac{1}{MN}\sum_{m=1}^{M}\sum_{n=1}^{N}\left|\log_{q}\left(\left|X_{mn}\right|\right)\right|
\end{equation}
Aby uprościć zagadnienie pominięto nieistotne z punktu widzenia złożoności
mnożenie przez stałą $\frac{1}{MN}$ oraz przestawiono problem na przykładzie
wektora $\bm x$ o długości $K=MN$, którego elementy $x_k = \left|X_{mn}\right|$
są modułami elementów macierzy $\bm X$  w dowolnej kolejności
\begin{equation}\label{eq:naive_algorithm}
  \left<x\right> = \sum_{k=1}^{K}\left|\log_{q}\left(x_{k}\right)\right|.
\end{equation}
Korzystając z własności modułu i własności logarytmu:
\begin{equation}
  |a| + |b| = \begin{cases}
    |a + b|, & (a \geq 0 \land b \geq 0) \lor (a < 0 \land b < 0)\\
    |a - b|, & (a > 0 \land b < 0) \lor (a < 0 \land b > 0)\\
  \end{cases},
\end{equation}
\begin{equation}
  \log_q(a) + \log_q(b) = \log_q(ab),
\end{equation}
\begin{equation}
  \log_q(a) - \log_q(b) = \log_q\left(\frac{a}{b}\right),
\end{equation}
\begin{equation}
  \log_q(a) \geq 0 \iff a \geq 1 \land \log_q(a) \leq 0 \iff a \leq 1 .
\end{equation}
Można wywnioskować:
\begin{equation}
  |\log_q(a)| + |\log_q(b)| = \begin{cases}
    |\log_q(ab)|, & (a \geq 1 \land b \geq 1) \lor (a < 1 \land b < 1)\\
    |\log_q\left(\frac{a}{b}\right)|, & (a > 1 \land b < 1) \lor (a < 1 \land b > 1)\\
  \end{cases}.
\end{equation}
Z czego, określiwszy operator $\odot$ jako
\begin{equation}\label{eq:odot_operation}
  a \odot b = \begin{cases}
    ab, & (a \geq 1 \land b \geq 1) \lor (a < 1 \land b < 1)\\
    \frac{a}{b}, & (a > 1 \land b < 1) \lor (a < 1 \land b > 1)\\
  \end{cases},
\end{equation}
wyprowadzono następujący algorytm:
\begin{equation}
  \left<x\right> = |\log_q(\overline{x_K})|, \quad\text{gdzie}\quad
  \begin{cases}
    \overline{x_1} &= x_1\\
    \overline{x_{k+1}} &= \overline{x_k}\odot x_{k+1}
  \end{cases},
\end{equation}

Jak widać w wersji ,,naiwnej'' (\ref{eq:naive_algorithm}) aby policzyć
$\left<x\right>$ należy wykonać aż $K$ operacji logarytmowania, która jest
kosztowna pod względem obliczeniowym. W usprawnionej wersji algorytmu wystarczy
jedna operacja logarytmowania bez względu na rozmiar wektora wejściowego.
Natomiast ta prosta wersja algorytmu jest bezużyteczna dla dużych wartości $K$
z powodu ograniczonej precyzji zmiennych zmiennoprzecinkowych. Im większa
wartość $K$ tym większa szansa na wykorzystanie dostępnej precyzji co objawia
się uzyskaniem z operacji $a \odot b$ wartości $0$ lub $\pm\infty$, dla
$a \neq 0 \land b \neq 0$. Wobec tego opracowano algorytm:
\begin{enumerate}
  \item Mając wektor początkowy $\bm x$ o długość $N$, należy wyznaczyć wektor
    pośredni $\bm y$ o długości
    $\left\lceil\frac{N}{2}\right\rceil$, którego elementy są wynikami
    zastosowania operacji $\odot$ (\ref{eq:odot_operation}) na kolejnych parach
    elementów wektora $\bm x$, tak że \begin{equation}
      y_i = \begin{cases}
        x_{2i-1}\odot x_{2i}, & 2i \leq N\\
        x_{2i-1}, & 2i > N
      \end{cases}
    \end{equation}
  \item Należy powtarzać krok pierwszy zastępując wektor początkowy wektorem
    wynikowym, do czasu aż wektor wynikowy będzie miał długość 1, lub skończy
    się precyzja zmiennych, po czym na uzyskanym w ten sposób wektorze
    zastosować ,,naiwny'' algorytm (\ref{eq:naive_algorithm}).
\end{enumerate}

Ta metodyka, w zależności od rozmiaru wektora i danych w nim zawartych
zmniejsza ilość operacji logarytmowania potrzebnych do wyliczenia
$\left<x\right>$. Na rysunku \ref{fig:sum_abs_log_abs} przedstawiony jest
stosunek czasu wykonywania algorytmu ,,naiwnego'' do tego algorytmu. Można
zauważyć że jest on najefektywniejszy dla rozmiarów wektora pomiędzy
10\textsuperscript{4}, a 10\textsuperscript{5} elementów, gdzie wykonuję tę samą
operację w czasie cztery razy krótszym niż algorytm ,,naiwny'', lecz nawet dla
rozmiarów wektora powyżej 10\textsuperscript{6} oferuje on czas wykonywania co
najmniej \num{2.5} raza krótszy.
\begin{figure}[!tbh]
  \centering
  \includegraphics[width=0.6\textwidth]{plots/sum_abs_log10_abs.pgf}
  \caption{Wydajność algorytmu}
  \label{fig:sum_abs_log_abs}
\end{figure}

\subsection{MDCT}
\section{Uczenie maszynowe}
Zagadnienie rozróżniania sygnałów audio przedstawiono w postaci zagadnienia
które jest jednym z podstawowych zagadnień na których skupia się uczenie
maszynowe, mianowicie chodzi o zagadnienie rozpoznawania obrazów. Sygnał audio
można przedstawić w postaci spektrogramu, który pozwala nie tylko rozdzielić
sygnał na pasma częstotliwościowe, dzięki czemu na obrazie znajdują się
informacje czasowo-częstotliwościowe sygnału.
\subsection{Konwolucyjne Sieci Neuronowe}
Konwolucyjne sieci neuronowe (eng. \textit{CNN -- Convolutional Neural
Networks}) typowo posiadają trzy typy warstw: warstwy konwolucyjne,
warstwy podpróbkujące oraz warstwy w pełni połączone. Są one stosowane do
rozpoznawania obrazów, gdyż ich konstrukcja skutecznie wyselekcjonowuje istotne
informacje z obrazów podobnie jak oczy ludzkie.

Warstwy konwolucyjne wykonują operację splotu wejściowych tensorów z jądrem
(nazywanym też filtrem) --- tensorem dwuwymiarowym o elementach (tak zwanych
wagach) które są optymalizowane w procesie nauki sieci. Mając tensor wejściowy
o wymiarach $W \times H \times D$, odpowiadających szerokości, wysokości i
głębokości (liczbie kanałów) obrazu wejściowego, oraz kolekcje jąder $M \times
N$ o wymiarach $D \times T \times M \times N$, wpierw wykonywany jest tzw.
\textit{padding} --- proces w którym tensor wejściowy zwiększany jest o pewną
liczbę $P$ zer na krawędziach dwóch pierwszych wymiarów uzyskując tensor
wymiarach $W+2P \times H+2P \times D$. Następnie każdy z kanałów obrazu
wejściowego o wymiarach $W+2P \times H+2P$ jest splatany z każdym kolejnym
jądrem dla danego kanału od 1 do $T$ uzyskując w efekcie $D\times T$ tensorów
wynikowych splotu. Te tensory są następnie sumowane po wymiarze $D$ uzyskując
$T$ tensorów wynikowych.

W procesie pojedynczego splotu macierz jądra jest mnożona przez wycinek kanału
obrazu o wymiarach jądra poczynając od lewej górnej krawędzi i przesuwając się
w prawo o skok $S$ aż do krawędzi po czym zaczynając z powrotem od lewej
przesunąwszy się w dół o $S$. Proces ten przedstawia się następująco na
przykładzie obrazu $\bm O$ o wymiarach $4 \times 4 \times 1$ i jadra $\bm J$ o
wymiarach $3 \times 3$, padzie $P = 0$ i skoku $S = 1$, gdzie zaznaczono
elementy macierzy biorące udział w działaniu:

\begin{equation}
  \bm O = \begin{bmatrix}
    3 & 3 & 2 & 1\\
    0 & 0 & 1 & 3\\
    3 & 1 & 2 & 2\\
    2 & 0 & 0 & 2
  \end{bmatrix},\quad
  \bm J = \begin{bmatrix}
    0 & 1 & 2\\
    2 & 2 & 0\\
    0 & 1 & 2
  \end{bmatrix},
\end{equation}
\tikzset{external/export=false}
\begin{equation}
  \begin{array}{*2{c}}
    \left[\begin{array}{*4{c}}
      \tikzmark{left}{3} & 3 & 2 & 1\\
      0 & 0 & 1 & 3\\
      3 & 1 & \tikzmark{right}{2} & 2\\
      2 & 0 & 0 & 2
    \end{array}\right]\times \bm J =
    \Highlight
    \left[\begin{array}{*2{c}}
        \tikzmark{left}{12}\tikzmark{right}{12} & \phantom{12}\\
        \phantom{10} & \phantom{17}
    \end{array}\right]
    \Highlight&
    \left[\begin{array}{*4{c}}
      3 & \tikzmark{left}{3} & 2 & 1\\
      0 & 0 & 1 & 3\\
      3 & 1 & 2 & \tikzmark{right}{2}\\
      2 & 0 & 0 & 2
    \end{array}\right]\times \bm J =
    \Highlight
    \left[\begin{array}{*2{c}}
        12 & \tikzmark{left}{12}\tikzmark{right}{12}\\
        \phantom{10} & \phantom{17}
    \end{array}\right]
    \Highlight\\
    \left[\begin{array}{*4{c}}
      3 & 3 & 2 & 1\\
      \tikzmark{left}{0} & 0 & 1 & 3\\
      3 & 1 & 2 & 2\\
      2 & 0 & \tikzmark{right}{0} & 2
    \end{array}\right]\times \bm J =
    \Highlight
    \left[\begin{array}{*2{c}}
        12 & 12\\
        \tikzmark{left}{10}\tikzmark{right}{10} & \phantom{17}
    \end{array}\right]
    \Highlight&
    \left[\begin{array}{*4{c}}
      3 & 3 & 2 & 1\\
      0 & \tikzmark{left}{0} & 1 & 3\\
      3 & 1 & 2 & 2\\
      2 & 0 & 0 & \tikzmark{right}{2}
    \end{array}\right]\times \bm J =
    \Highlight
    \left[\begin{array}{*2{c}}
      12 & 12\\
      10 & \tikzmark{left}{17}\tikzmark{right}{17}
    \end{array}\right]
    \Highlight\\
  \end{array}.
\end{equation}
\tikzset{external/export=true}
Wynikiem tej operacji jest macierz $\bm C$ o wymiarach $2\times 2$:
\begin{equation}
  \bm C = \begin{bmatrix}
    12 & 12\\
    10 & 17
  \end{bmatrix}.
\end{equation}

Kolejnym krokiem po konwolucji jest podpróbkowanie które zmniejsza rozmiar
tensora kilkukrotnie zachowując najistotniejsze informacje. Tzw okno --- analog
jądra w konwolucji o pewnym wymiarze przesuwa się ze skokiem $S$ wybierając z
zaznaczonej części macierzy wartość maksymalną, w przypadku algorytmu
\textit{Max Pooling}, lub średnią, w przypadku algorytmu \textit{Average
Pooling}. Dla każdego kanału obrazu ten proces jest wykonywany niezależnie.
Przykład działania \textit{Max Pooling} na macierzy $\bm O$ z oknem $2 \times
2$ i skokiem $S = 2$ przedstawia się następująco:
\tikzset{external/export=false}
\begin{equation}
  \begin{array}{*2{c}}
    \max\left\{\left[\begin{array}{*4{c}}
      \tikzmark{left}{3} & 3 & 2 & 1\\
      0 & \tikzmark{right}{0} & 1 & 3\\
      3 & 1 & 2 & 2\\
      2 & 0 & 0 & 2
    \end{array}\right]\right\} =
    \Highlight
    \left[\begin{array}{*2{c}}
        \tikzmark{left}{3}\tikzmark{right}{3} & \phantom{3}\\
        \phantom{3} & \phantom{2}
    \end{array}\right]
    \Highlight&
    \max\left\{\left[\begin{array}{*4{c}}
      3 & 3 & \tikzmark{left}{2} & 1\\
      0 & 0 & 1 & \tikzmark{right}{3}\\
      3 & 1 & 2 & 2\\
      2 & 0 & 0 & 2
\end{array}\right]\right\} =
    \Highlight
    \left[\begin{array}{*2{c}}
        3 & \tikzmark{left}{3}\tikzmark{right}{3}\\
        \phantom{3} & \phantom{2}
    \end{array}\right]
    \Highlight\\
    \max\left\{\left[\begin{array}{*4{c}}
      3 & 3 & 2 & 1\\
      0 & 0 & 1 & 3\\
      \tikzmark{left}{3} & 1 & 2 & 2\\
      2 & \tikzmark{right}{0} & 0 & 2
\end{array}\right]\right\}=
    \Highlight
    \left[\begin{array}{*2{c}}
        3 & 3\\
        \tikzmark{left}{3}\tikzmark{right}{3} & \phantom{2}
    \end{array}\right]
    \Highlight&
    \max\left\{\left[\begin{array}{*4{c}}
      3 & 3 & 2 & 1\\
      0 & 0 & 1 & 3\\
      3 & 1 & \tikzmark{left}{2} & 2\\
      2 & 0 & 0 & \tikzmark{right}{2}
\end{array}\right]\right\} =
    \Highlight
    \left[\begin{array}{*2{c}}
      3 & 3\\
      3 & \tikzmark{left}{2}\tikzmark{right}{2}
    \end{array}\right]
    \Highlight\\
  \end{array}.
\end{equation}
\tikzset{external/export=true}
W wyniku uzyskana jest macierz $\bm M$ o wymiarach $2\times 2$:
\begin{equation}
  \bm M = \begin{bmatrix}
    3 & 3\\
    3 & 2
  \end{bmatrix}
\end{equation}
\begin{figure}[!tbh]
  \centering
  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
    \tikzset{%
      every neuron/.style={
        circle,
        draw,
        minimum size=1cm
      },
      neuron missing/.style={
        draw=none, 
        scale=3,
        text height=0.333cm,
        execute at begin node=\color{black}$\vdots$
      },
      neuron missing2/.style={
        draw=none, 
        scale=3,
        text height=0.222cm,
        execute at begin node=\color{black}$\cdots$
      },
      neuron none/.style={
        draw=none
      }
    }
    \foreach \m/\l [count=\y] in {1,2,3,missing,4}{
      \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};
    }
    \foreach \m [count=\y] in {1,2,missing,3}{
      \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (1.5,2-\y) {};
    }
    \foreach \m [count=\y] in {missing2,missing2,none,missing2}{
      \node [every neuron/.try, neuron \m/.try ] (dottes-\m) at (2.55,2-\y) {};
    }
    \foreach \m [count=\y] in {1,2,missing,3}{
      \node [every neuron/.try, neuron \m/.try ] (hidden2-\m) at (3.5,2-\y) {};
    }
    \foreach \m [count=\y] in {1,missing,2}{
      \node [every neuron/.try, neuron \m/.try ] (output-\m) at (5,1.5-\y) {};
    }
    \foreach \l [count=\i] in {1,2,3,n_0}{
      \draw [<-] (input-\i) -- ++(-1,0);
      \node [] at (input-\i) {$i_{\l}$};
    }
    \foreach \l [count=\i] in {1,2,n_1}{
      \node [] at (hidden-\i) {$h_{\l}^{(1)}$};
    }
    \foreach \l [count=\i] in {1,2,n_m}{
      \node [] at (hidden2-\i) {$h_{\l}^{(m)}$};
    }
    \foreach \l [count=\i] in {1,n_{m+1}}{
      \draw [->] (output-\i) -- ++(1,0);
      \node [] at (output-\i) {$o_{\l}$};
    }
    \foreach \i in {1,...,4}{
      \foreach \j in {1,...,3}{
        \draw [->] (input-\i) -- (hidden-\j);
      }
    }
    \foreach \i in {1,...,3}{
      \foreach \j in {1,...,2}{
        \draw [->] (hidden2-\i) -- (output-\j);
      }
    }
    \foreach \l [count=\x from 0] in {Dane\\wejściowe,Warstwy\\ukryte,Warstwa\\wyjściowa}{
      \node [align=center, above] at (\x*2.5,2) {\l};
    }
  \end{tikzpicture}
  \caption{Schemat sieci w pełni połączonej}
  \label{fig:fully_connected}
\end{figure}
Trzecim typem warst stosowanych w sieciach CNN są warstwy w pełni połączone
(eng. \textit{FC -- Fully Connected}). Tego typu warstwy umieszcza się na
końcu łańcucha przetwarzania, ich zadaniem jest rozpoznanie kategorii obrazu na
podstawie cech wyseparowanych przez poprzednie warstwy konwolucyjne i
podpróbkujące. 
Schematycznie warstwy w pełni połączone przedstawia rysunek
\ref{fig:fully_connected}, sieć na nim posiada $m$ warstw ukrytych oraz warstwę
wyjściową. Aby uzyskać element wektora pierwszej warstwy ukrytej $\bm h^{(1)}$,
należy wymnożyć każdy z elementów wektora danych wyjściowych $\bm i$ przez
odpowiadający mu element wektora wag $\bm w_{0}$, zsumować wyniki mnożenia i
na wyniku wykonać funkcję nieliniową $\sigma_0$, zwaną funkcją aktywacji,
\begin{equation}
  h_k^{(1)} = \sigma_0\left(\sum_{j=1}^{n_0}i_jw_{0,k,j}\right).
\end{equation}
Analogicznie wartości kolejnych warstw ukrytych wynoszą:
\begin{equation}
  h_k^{(p)} = \sigma_{p-1}\left(\sum_{j=1}^{n_{p-1}}h_j^{(p-1)}w_{p-1,k,j}\right).
\end{equation}

W zagadnieniach rozpoznawania obrazów jako funkcję aktywacji $\sigma$ zazwyczaj
stosuje się funkcję reLU (eng. \textit{Rectified Linear Unit}) $f(x) =
\max(0,x)$, której wykres przedstawia rysunek \ref{fig:reLU}. Pozwala ona
uzyskać tą samą dokładność w czasie kilkukrotnie krótszym niż tradycyjnie
stosowane funkcje $f(x) = \tanh(x)$ lub $f(x) = (1 + e^{-x})^{-1}$. Ta sama
funkcja stosowana jest także na wyjściach warstw podpróbkujących.

\begin{figure}[!tbh]
  \centering
  \includegraphics[width=0.5\textwidth]{plots/relu.pgf}
  \caption{Funkcja aktywacji reLU}
  \label{fig:reLU}
\end{figure}

Wektory wag $\bm w_0,\dotsc,\bm w_m$ podlegają optymalizacji w procesie
uczenia, podobnie jak jądra w warstwach konwolucyjnych. Odbywa się to poprzez
wykonanie warstw sieci na danych treningowych i zastosowanie jednego z
algorytmów optymalizacyjnych, który, na podstawie tzw.\ funkcji strat modyfikuje wagi.
Kolejne iteracje tego procesu prowadzą do coraz dokładniejszych wyników
uzyskiwanych z sieci.

\subsection{Biblioteka Knet.jl}
\subsection{Konstrukcja modelu}
Model oparto na {\color{red}odnośnik}. Składa się z trzech warstw
konwolucyjnych o rozmiarze jądra $3 \times 3$, przy czym pierwsza posiada $1
\times 16$ takich jąder, a kolejne $16\times 16$. Po każdej następuje warstwa
podpróbkująca poprzez wybieranie wartości maksymalnej z przestrzeni $2 \times
2$ (\textit{Max Pooling}). Po tych warstwach następują dwie warstwy ukryte w
pełni połączone o trzystu węzłach i finalna warstwa w pełni połączona o liczbie
węzłów równej liczbie rozpoznawanych kategorii. Ponadto warstwy w pełni
połączone korzystają z wyłączania (dropout), pierwsza z prawdopodobieństwem $p
= 0.3$, a dwie kolejne z prawdopodobieństwem $p = 0.2$.  Wszystkie warstwy z
pominięciem ostatniej korzystają z funkcji aktywacji reLU. Rys.
\ref{fig:cnn_scheme} przedstawia tę architekturę sieci.
\begin{figure}[!tbh]
  \centering
  \scriptsize\import{vecgraphics/}{cnn-scheme2.pdf_tex}
  \caption{Model sieci konwolucyjnej}
  \label{fig:cnn_scheme}
\end{figure}
\subsection{Dane treningowe}
\chapter{Ewaluacja}
\section{Identyfikacja algorytmiczna}
\section{Identyfikacja modelem}
\subsection{Identyfikacja kompresji}
\subsection{Identyfikacja typu kompresji}
\subsection{Identyfikacja parametrów kompresji}
\subsection{Identyfikacja typu i parametrów kompresji}
\chapter{Podsumowanie}
\section{Wnioski końcowe}
\section{Dalsze kierunki badań}

%Kod poniżej dodaje Bibliografię do spisu treści
\cleardoublepage{}
\phantomsection{}
\addcontentsline{toc}{chapter}{Bibliografia}
\printbibliography{}

\end{document}
